# Auto-generated by ChatGPT
# Usage:
#   python model_schema_matcher.py /path/to/dir_or_files... [--debug]
# Scans YAMLs, groups by "<N>-<ROLE>-*.yaml", extracts IO + AB pattern, and classifies matches.

import os
import re
import sys
import yaml
import pandas as pd
from typing import List, Dict, Any, Tuple, Optional, Set
from dataclasses import dataclass, field

# ------------------------------
# Text/Token utilities
# ------------------------------

STOPWORDS = [
    "a","an","and","as","at","beneath","by","canonical","component","components","coupled",
    "data","dataset","datasets","description","exchange","exchanges","expected","field","fields",
    "flux","fluxes","for","from","ice","in","input","inputs","intended","into","measurement",
    "measurements","model","models","ocean","of","on","or","output","outputs","rate","rates",
    "schema","sea","shelf","surface","the","to","under","variable","variables","with"
]

def normalize_phrase(s: str) -> str:
    if not s:
        return ""
    s = s.replace("–", "-")
    s = re.sub(r"\s+and\s+", ", ", s, flags=re.IGNORECASE)
    s = s.replace("/", " / ")
    s = re.sub(r"[\(\)\[\]]", " ", s)  # drop bracket content (units) from "name" comparison
    s = re.sub(r"[:,;|]+", ",", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip().lower()

def split_variables(text: str) -> List[str]:
    text = normalize_phrase(text)
    if not text:
        return []
    parts = [p.strip() for p in text.split(",") if p.strip()]
    out: List[str] = []
    for p in parts:
        p2 = re.sub(r"\s+", " ", p).strip()
        if p2:
            out.append(p2)
    return out

def tokenize(s: str) -> Set[str]:
    s = normalize_phrase(s)
    tokens = re.findall(r"[a-zA-Z0-9\-\+°²/]+", s)
    return set(t for t in tokens if t not in STOPWORDS and len(t) > 1)

def phrase_similarity(a: str, b: str) -> float:
    if not a or not b:
        return 0.0
    if a == b:
        return 1.0
    if a in b or b in a:
        return 0.9
    ta, tb = tokenize(a), tokenize(b)
    if not ta or not tb:
        return 0.0
    inter = len(ta & tb)
    union = len(ta | tb)
    return inter / union if union else 0.0

def list_similarity(src: List[str], dst: List[str]) -> Tuple[float, List[Tuple[str, str, float]]]:
    if not src or not dst:
        return (0.0, [])
    matches = []
    scores: List[float] = []
    for s in src:
        best_b, best = "", 0.0
        for d in dst:
            sim = phrase_similarity(s, d)
            if sim > best:
                best, best_b = sim, d
        matches.append((s, best_b, best))
        scores.append(best)
    return ((sum(scores) / len(scores)) if scores else 0.0, matches)

def jaccard_token_similarity(a: str, b: str) -> float:
    ta, tb = tokenize(a), tokenize(b)
    if not ta or not tb:
        return 0.0
    inter = len(ta & tb)
    union = len(ta | tb)
    return inter / union if union else 0.0

def dedupe(items: List[str]) -> List[str]:
    seen, out = set(), []
    for x in items:
        if x not in seen:
            out.append(x)
            seen.add(x)
    return out

def coerce_var_list(v: Any) -> List[str]:
    if isinstance(v, str):
        return split_variables(v)
    if isinstance(v, list):
        out: List[str] = []
        for item in v:
            if isinstance(item, str):
                out.extend(split_variables(item))
        return dedupe(out)
    return []

def coerce_units_list(v: Any) -> List[str]:
    if isinstance(v, str):
        return [u.strip() for u in re.split(r"[;,]", v) if u.strip()]
    if isinstance(v, list):
        return [str(u).strip() for u in v if str(u).strip()]
    return []

# ------------------------------
# Dataclasses
# ------------------------------

@dataclass
class IOSchema:
    variables: List[str] = field(default_factory=list)
    units: List[str] = field(default_factory=list)

@dataclass
class ModelMeta:
    file: str
    group: str
    role: str
    name: str
    root: Dict[str, Any]
    outputs: IOSchema = field(default_factory=IOSchema)
    inputs: IOSchema = field(default_factory=IOSchema)
    canonical_exchange: List[str] = field(default_factory=list)  # union of AB_in + AB_out + integrated_input desc
    ab_in: List[str] = field(default_factory=list)
    ab_out: List[str] = field(default_factory=list)
    ab_pattern: str = ""  # integration pattern

# ------------------------------
# File parsing
# ------------------------------

def guess_group_role_from_filename(path: str) -> Tuple[str, str]:
    base = os.path.basename(path)
    m = re.match(r"^(\d+)-(A|B|AB)[-_]", base, flags=re.IGNORECASE)
    group = m.group(1) if m else "unknown"
    role = m.group(2).upper() if m else "unknown"
    return group, role

def load_yaml_models(paths: List[str]) -> List[ModelMeta]:
    models: List[ModelMeta] = []
    for p in paths:
        try:
            with open(p, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)
        except Exception as e:
            print(f"Warning: failed to parse {p}: {e}")
            continue
        if isinstance(data, dict) and data:
            name = list(data.keys())[0]
            root = data[name]
        else:
            name = os.path.splitext(os.path.basename(p))[0]
            root = data if isinstance(data, dict) else {}
        group, role = guess_group_role_from_filename(p)
        models.append(ModelMeta(file=p, group=group, role=role, name=name, root=root))
    return models

# ------------------------------
# AB pattern parsing
# ------------------------------

def parse_ab_pattern(r: Dict[str, Any]) -> str:
    """Detect integration pattern; recognizes 'Tool-Coupling' and synonyms."""
    candidates: List[str] = []
    integ = r.get("integration")
    if isinstance(integ, dict):
        pat = integ.get("pattern")
        if isinstance(pat, str):
            candidates.append(pat)
    pp = r.get("purpose_pattern")
    if isinstance(pp, dict):
        pat = pp.get("pattern")
        if isinstance(pat, str):
            candidates.append(pat)

    text = " ".join(candidates).lower()

    # New: Tool-Coupling (co-simulation / workflow / middleware)
    if ("tool coupling" in text or "tool-coupling" in text or "tools coupling" in text
        or "co-simulation" in text or "cosimulation" in text
        or "workflow" in text or "orchestrator" in text or "orchestration" in text
        or "middleware" in text or "coupler framework" in text):
        return "Tool-Coupling"

    if "embedded" in text:
        return "Embedded"
    if "integrated" in text or "tight" in text:
        return "Integrated"
    if "shared" in text or "canonical" in text:
        return "Shared"
    if "loose" in text or "adapter" in text or "adaptor" in text:
        return "Loose"
    if "one-way" in text or "one way" in text:
        return "One-Way"
    return ""  # unspecified

# ------------------------------
# IO extraction
# ------------------------------

def extract_io(model: ModelMeta, debug: bool = False) -> None:
    r = model.root if isinstance(model.root, dict) else {}

    # Outputs
    out_vars, out_units = [], []
    if isinstance(r.get("output"), dict):
        oo = r["output"]
        out_vars = coerce_var_list(oo.get("variables"))
        out_units = coerce_units_list(oo.get("units"))
    if not out_vars and isinstance(r.get("data"), dict):
        d = r["data"]
        out_vars = coerce_var_list(d.get("variables"))
        out_units = coerce_units_list(d.get("units"))

    # Inputs
    in_vars, in_units = [], []
    if isinstance(r.get("input"), dict):
        ii = r["input"]
        in_vars.extend(coerce_var_list(ii.get("variables")))
        in_units.extend(coerce_units_list(ii.get("units")))
        if isinstance(ii.get("description"), str):
            in_vars.extend(split_variables(ii["description"]))

    if isinstance(r.get("integrated_input"), list):
        for item in r["integrated_input"]:
            if isinstance(item, dict):
                in_vars.extend(split_variables(item.get("description", "")))

    if isinstance(r.get("datasets"), list):
        for ds in r["datasets"]:
            if isinstance(ds, dict) and isinstance(ds.get("dependencies"), list):
                for dep in ds["dependencies"]:
                    if isinstance(dep, str):
                        in_vars.extend(split_variables(dep))

    model.outputs = IOSchema(variables=dedupe(out_vars), units=dedupe(out_units))
    model.inputs  = IOSchema(variables=dedupe(in_vars), units=dedupe(in_units))

    # AB extras
    if model.role == "AB":
        model.ab_pattern = parse_ab_pattern(r)
        if isinstance(r.get("input"), dict):
            model.ab_in = dedupe(coerce_var_list(r["input"].get("variables")))
        if isinstance(r.get("output"), dict):
            model.ab_out = dedupe(coerce_var_list(r["output"].get("variables")))
        canon: List[str] = []
        if isinstance(r.get("integrated_input"), list):
            for item in r["integrated_input"]:
                if isinstance(item, dict):
                    canon.extend(split_variables(item.get("description", "")))
        canon.extend(model.ab_in)
        canon.extend(model.ab_out)
        # fallbacks: scrape helpful text if canonical empty
        if not canon:
            for k in ("description", "purpose_pattern", "integration"):
                val = r.get(k, "")
                if isinstance(val, dict):
                    for vv in val.values():
                        if isinstance(vv, str):
                            canon.extend(split_variables(vv))
                elif isinstance(val, str):
                    canon.extend(split_variables(val))
        model.canonical_exchange = dedupe(canon)

    if debug:
        print(f"\n[DEBUG] {model.file} ({model.role})")
        print(f"  outputs: {model.outputs.variables or '(none)'}")
        print(f"  inputs:  {model.inputs.variables or '(none)'}")
        if model.role == "AB":
            print(f"  AB in:   {model.ab_in or '(none)'}")
            print(f"  AB out:  {model.ab_out or '(none)'}")
            print(f"  AB pat:  {model.ab_pattern or '(unspecified)'}")
            print(f"  AB canon:{model.canonical_exchange or '(none)'}")

# ------------------------------
# Overlap helpers (counts + names)
# ------------------------------

T_MIN = 0.5  # minimum phrase similarity to count as an overlapping variable

def overlap_matches(src: List[str], dst: List[str], min_sim: float = T_MIN) -> Tuple[int, List[Tuple[str, str, float]]]:
    """Return (count, list) where list includes (src, best_dst, sim) for sim >= min_sim."""
    _, pairs = list_similarity(src, dst)
    good = [(s, d, sc) for (s, d, sc) in pairs if sc >= min_sim]
    return len(good), good

def format_matched_vars(pairs: List[Tuple[str, str, float]]) -> str:
    if not pairs:
        return ""
    return "; ".join([f"{s} ↔ {d} ({sc:.2f})" for (s, d, sc) in pairs])

# ------------------------------
# Classification (zero-overlap => Mismatch)
# ------------------------------

def classify_per_rules_counts(
    ab_pattern: str,
    AtoB_count: int,
    BtoA_count: int,
    units_sim: Optional[float],
    s_Ain_to_ABin: Optional[float],
    s_Bout_to_ABout: Optional[float],
    canon_present: bool
) -> Tuple[str, str]:
    """
    Mismatch only if required direction(s) have zero overlap.
    - One-Way: require A->B_count >= 1
    - Tool-Coupling / Loose / Shared / Integrated / Embedded / Unspecified: require BOTH counts >= 1
    For Tool-Coupling we report 'Loose' and note that middleware can adapt.
    """
    rationale: List[str] = []

    requires_two_way = (ab_pattern not in ("One-Way",))

    if requires_two_way:
        if AtoB_count == 0 and BtoA_count == 0:
            return "Mismatch", "No common variables in either direction."
        if AtoB_count == 0:
            return "Mismatch", "No common variables from A.outputs to B.inputs."
        if BtoA_count == 0:
            return "Mismatch", "No common variables from B.outputs to A.inputs."
    else:
        if AtoB_count == 0:
            return "Mismatch", "No common variables from A.outputs to B.inputs for One-Way integration."

    units_ok = (units_sim is None) or (units_sim >= 0.8)

    if ab_pattern == "Tool-Coupling":
        level = "Loose"
        rationale.append("Two-way exchange feasible; middleware/coupler can adapt names/units as needed.")
    elif not requires_two_way:
        level = "One-Way"
        rationale.append("One-way exchange feasible with overlapping variables.")
    else:
        level = "Loose"
        rationale.append("Two-way exchange feasible with overlapping variables.")
        if units_ok:
            rationale.append("Units appear compatible or unspecified.")

    if s_Ain_to_ABin is not None:
        rationale.append(f"A.input vs AB.input similarity={s_Ain_to_ABin:.2f}.")
    if s_Bout_to_ABout is not None:
        rationale.append(f"B.output vs AB.output similarity={s_Bout_to_ABout:.2f}.")
    if canon_present:
        rationale.append("AB canonical present (used for context).")

    return level, " ".join(rationale)

# ------------------------------
# Compute scores per group
# ------------------------------

def compute_scores(a: ModelMeta, b: ModelMeta, ab: Optional[ModelMeta]) -> Dict[str, Any]:
    # Overlaps (counts + matched pairs)
    AtoB_count, AtoB_pairs = overlap_matches(a.outputs.variables, b.inputs.variables, T_MIN)
    BtoA_count, BtoA_pairs = overlap_matches(b.outputs.variables, a.inputs.variables, T_MIN)

    # Similarities (for reference)
    s_Aout_to_Bin, _ = list_similarity(a.outputs.variables, b.inputs.variables)
    s_Bout_to_Ain, _ = list_similarity(b.outputs.variables, a.inputs.variables)

    # Units similarity if provided
    units_a = ", ".join(a.outputs.units)
    units_b = ", ".join(b.inputs.units)
    u_Aout_Bin = jaccard_token_similarity(units_a, units_b) if (units_a and units_b) else None

    # Optional AB constraints
    ab_in = ab.ab_in if (ab and ab.ab_in) else []
    ab_out = ab.ab_out if (ab and ab.ab_out) else []
    s_Ain_to_ABin = (list_similarity(a.inputs.variables, ab_in)[0] if ab_in else None)
    s_Bout_to_ABout = (list_similarity(b.outputs.variables, ab_out)[0] if ab_out else None)

    canon = ab.canonical_exchange if ab else []
    canon_present = bool(canon)

    ab_pattern = ab.ab_pattern if ab else ""
    level, rationale = classify_per_rules_counts(
        ab_pattern=ab_pattern,
        AtoB_count=AtoB_count,
        BtoA_count=BtoA_count,
        units_sim=u_Aout_Bin,
        s_Ain_to_ABin=s_Ain_to_ABin,
        s_Bout_to_ABout=s_Bout_to_ABout,
        canon_present=canon_present
    )

    return {
        # Names
        "A_name": a.name,
        "B_name": b.name,
        "AB_name": (ab.name if ab else ""),
        # Interfaces (required columns)
        "A_input": "; ".join(a.inputs.variables) or "(none found)",
        "B_input": "; ".join(b.inputs.variables) or "(none found)",
        "A_output": "; ".join(a.outputs.variables) or "(none found)",
        "B_output": "; ".join(b.outputs.variables) or "(none found)",
        "AB_input": "; ".join(ab_in) if ab_in else "(not specified)",
        "AB_output": "; ".join(ab_out) if ab_out else "(not specified)",
        "integration_pattern": (ab_pattern or "(unspecified)"),
        # Overlap diagnostics
        "AtoB_match_count": AtoB_count,
        "AtoB_matched_vars": format_matched_vars(AtoB_pairs),
        "BtoA_match_count": BtoA_count,
        "BtoA_matched_vars": format_matched_vars(BtoA_pairs),
        # Similarity metrics (reference)
        "sim_Aout_to_Bin": round(s_Aout_to_Bin, 3),
        "sim_Bout_to_Ain": round(s_Bout_to_Ain, 3),
        "sim_Ain_to_ABin": (None if s_Ain_to_ABin is None else round(s_Ain_to_ABin, 3)),
        "sim_Bout_to_ABout": (None if s_Bout_to_ABout is None else round(s_Bout_to_ABout, 3)),
        "units_sim_Aout_Bin": (None if u_Aout_Bin is None else round(u_Aout_Bin, 3)),
        # Final classification
        "classification": level,
        "rationale": rationale,
    }

# ------------------------------
# CLI main
# ------------------------------

def main(args: List[str]) -> None:
    debug = False
    filtered_args: List[str] = []
    for a in args:
        if a == "--debug":
            debug = True
        else:
            filtered_args.append(a)

    # Prefer 'metamodel' if present and no args
    paths: List[str] = []
    default_root = "metamodel" if os.path.isdir("metamodel") else "."
    scan_roots = filtered_args or [default_root]

    for a in scan_roots:
        if os.path.isdir(a):
            for root, _, files in os.walk(a):
                for fn in files:
                    if fn.lower().endswith((".yml", ".yaml")):
                        paths.append(os.path.join(root, fn))
        else:
            paths.append(a)

    if not paths:
        print("No YAML files found.")
        return

    models = load_yaml_models(paths)
    for m in models:
        try:
            extract_io(m, debug=debug)
        except Exception as e:
            print(f"Warning: failed to extract IO for {m.file}: {e}")

    # Group by numeric prefix
    from collections import defaultdict
    groups: Dict[str, List[ModelMeta]] = defaultdict(list)
    for m in models:
        groups[m.group].append(m)

    rows: List[Dict[str, Any]] = []
    for gid, items in sorted(groups.items()):
        A = next((x for x in items if x.role == "A"), None)
        B = next((x for x in items if x.role == "B"), None)
        ABs = [x for x in items if x.role == "AB"]
        AB = sorted(ABs, key=lambda x: ("integrated" not in x.file.lower(), x.file.lower()))[0] if ABs else None
        if A and B:
            try:
                rows.append(compute_scores(A, B, AB))
            except Exception as e:
                print(f"Warning: failed to score group {gid}: {e}")

    if not rows:
        print("No A/B pairs found to score.")
        return

    col_order = [
        "A_name","B_name","AB_name",
        "A_input","B_input","A_output","B_output","AB_input","AB_output","integration_pattern",
        "AtoB_match_count","AtoB_matched_vars","BtoA_match_count","BtoA_matched_vars",
        "sim_Aout_to_Bin","sim_Bout_to_Ain","sim_Ain_to_ABin","sim_Bout_to_ABout","units_sim_Aout_Bin",
        "classification","rationale"
    ]
    df = pd.DataFrame(rows)
    df = df[[c for c in col_order if c in df.columns] + [c for c in df.columns if c not in col_order]]

    out_csv = "match_report.csv"
    df.to_csv(out_csv, index=False)
    print(df.to_string(index=False))
    print("\nSaved CSV:", out_csv)

if __name__ == "__main__":
    main(sys.argv[1:])
