# Auto-generated by ChatGPT — revised to check all bottlenecks and emit a per-bottleneck/field report
# Usage:
#   python model_schema_matcher.py /path/to/dir_or_files... [--debug]
# Scans YAMLs, groups by "<N>-<ROLE>-*.yaml", extracts metadata, infers AB pattern,
# evaluates all bottlenecks, and writes a long-form CSV "match_report.csv".

import os
import re
import sys
import yaml
import math
import pandas as pd
from typing import List, Dict, Any, Tuple, Optional, Set, Iterable
from dataclasses import dataclass, field
from collections import defaultdict

# ============================================================
# Text/Token utilities
# ============================================================

STOPWORDS = {
    "a","an","and","as","at","beneath","by","canonical","component","components","coupled",
    "data","dataset","datasets","description","exchange","exchanges","expected","field","fields",
    "flux","fluxes","for","from","ice","in","input","inputs","intended","into","measurement",
    "measurements","model","models","ocean","of","on","or","output","outputs","rate","rates",
    "schema","sea","shelf","surface","the","to","under","variable","variables","with"
}

def normalize_phrase(s: str) -> str:
    if not s:
        return ""
    s = s.replace("–", "-")
    s = re.sub(r"\s+and\s+", ", ", s, flags=re.IGNORECASE)
    s = s.replace("/", " / ")
    s = re.sub(r"[\(\)\[\]]", " ", s)  # drop bracket content (units) from "name" comparison
    s = re.sub(r"[:,;|]+", ",", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip().lower()

def split_variables(text: str) -> List[str]:
    text = normalize_phrase(text)
    if not text:
        return []
    parts = [p.strip() for p in text.split(",") if p.strip()]
    out: List[str] = []
    for p in parts:
        p2 = re.sub(r"\s+", " ", p).strip()
        if p2:
            out.append(p2)
    return out

def tokenize(s: str) -> Set[str]:
    s = normalize_phrase(s)
    tokens = re.findall(r"[a-zA-Z0-9\-\+°²/]+", s)
    return {t for t in tokens if t not in STOPWORDS and len(t) > 1}

def phrase_similarity(a: str, b: str) -> float:
    if not a or not b:
        return 0.0
    if a == b:
        return 1.0
    if a in b or b in a:
        return 0.9
    ta, tb = tokenize(a), tokenize(b)
    if not ta or not tb:
        return 0.0
    inter = len(ta & tb)
    union = len(ta | tb)
    return inter / union if union else 0.0

def list_best_pairwise(src: List[str], dst: List[str]) -> Tuple[float, List[Tuple[str, str, float]]]:
    """Average of best matches from src to dst + list of (src, best_dst, score)."""
    if not src or not dst:
        return (0.0, [])
    matches = []
    scs: List[float] = []
    for s in src:
        best_b, best = "", 0.0
        for d in dst:
            sim = phrase_similarity(s, d)
            if sim > best:
                best, best_b = sim, d
        matches.append((s, best_b, best))
        scs.append(best)
    return ((sum(scs)/len(scs)) if scs else 0.0, matches)

def jaccard_token_similarity(a: str, b: str) -> float:
    ta, tb = tokenize(a), tokenize(b)
    if not ta or not tb:
        return 0.0
    inter = len(ta & tb)
    union = len(ta | tb)
    return inter / union if union else 0.0

def dedupe(items: Iterable[str]) -> List[str]:
    seen, out = set(), []
    for x in items:
        if x not in seen:
            out.append(x)
            seen.add(x)
    return out

def as_list(v: Any) -> List[str]:
    if v is None:
        return []
    if isinstance(v, list):
        flat: List[str] = []
        for it in v:
            if isinstance(it, (str, int, float)):
                flat.append(str(it))
            elif isinstance(it, dict):
                # join dict values as text
                for vv in it.values():
                    if isinstance(vv, (str, int, float)):
                        flat.append(str(vv))
        return dedupe([normalize_phrase(x) for x in flat if str(x).strip()])
    if isinstance(v, dict):
        flat: List[str] = []
        for vv in v.values():
            if isinstance(vv, (str, int, float)):
                flat.append(str(vv))
        return dedupe([normalize_phrase(x) for x in flat if str(x).strip()])
    return [normalize_phrase(str(v))]

# ============================================================
# Dataclasses
# ============================================================

@dataclass
class IOSchema:
    variables: List[str] = field(default_factory=list)
    units: List[str] = field(default_factory=list)

@dataclass
class ModelMeta:
    file: str
    group: str
    role: str
    name: str
    root: Dict[str, Any]
    outputs: IOSchema = field(default_factory=IOSchema)
    inputs: IOSchema = field(default_factory=IOSchema)
    ab_pattern: str = ""
    ab_in: List[str] = field(default_factory=list)
    ab_out: List[str] = field(default_factory=list)
    canonical_exchange: List[str] = field(default_factory=list)
    # generic bag of extracted fields for bottleneck checks
    fields: Dict[str, Any] = field(default_factory=dict)

# ============================================================
# Filename grouping
# ============================================================

def guess_group_role_from_filename(path: str) -> Tuple[str, str]:
    base = os.path.basename(path)
    m = re.match(r"^(\d+)-(A|B|AB)[-_]", base, flags=re.IGNORECASE)
    group = m.group(1) if m else "unknown"
    role = m.group(2).upper() if m else "unknown"
    return group, role

# ============================================================
# YAML loading
# ============================================================

def load_yaml_models(paths: List[str]) -> List[ModelMeta]:
    models: List[ModelMeta] = []
    for p in paths:
        try:
            with open(p, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)
        except Exception as e:
            print(f"Warning: failed to parse {p}: {e}")
            continue
        if isinstance(data, dict) and data:
            name = list(data.keys())[0]
            root = data[name]
        else:
            name = os.path.splitext(os.path.basename(p))[0]
            root = data if isinstance(data, dict) else {}
        group, role = guess_group_role_from_filename(p)
        models.append(ModelMeta(file=p, group=group, role=role, name=name, root=root or {}))
    return models

# ============================================================
# Generic recursive field extraction
# ============================================================

def normkey(k: str) -> str:
    return re.sub(r"[^a-z0-9]+", "_", k.strip().lower())

# Map normalized wanted field -> list of likely key aliases in YAML
FIELD_ALIASES: Dict[str, List[str]] = {
    # Enterprise
    "title": ["title","name"],
    "model_version": ["model_version","version","design_version"],
    "description": ["description","summary","abstract"],
    "keywords": ["keywords","tags"],
    "model_type": ["model_type","type"],
    "scope": ["scope"],
    "purpose_pattern": ["purpose_pattern","purpose_and_pattern","purpose","pattern"],
    "assumptions": ["assumptions"],
    "links_to_publications_and_reports": ["links","publications","reports","references","links_to_publications_and_reports"],
    "conceptual_model_evaluation": ["conceptual_model_evaluation","conceptual_evaluation"],
    "calibration_tools_data": ["calibration","calibration_tools","calibration_tools_data","calibration_data"],
    "validation_capabilities": ["validation_capabilities","validation"],
    "sensitivity_analysis": ["sensitivity_analysis","sensitivity"],
    "uncertainty_analysis": ["uncertainty_analysis","uncertainty"],
    "authors_unique_identifier": ["authors_unique_identifier","authors","contributors","author_ids","orcid"],
    "contributor_role": ["contributor_role","roles"],
    # Information
    "unique_identifier": ["unique_identifier","uuid","doi","id"],
    "parameters": ["parameters","params"],
    "datasets": ["datasets","data_sources"],
    "data": ["data","outputs","output_description"],
    "dimensionality": ["dimensionality","dims"],
    "spatial_extent_coverage": ["spatial_extent","spatial_coverage","extent","coverage"],
    "spatial_resolution": ["spatial_resolution","grid_resolution","grid_size"],
    "variable_spatial_resolution": ["variable_spatial_resolution","adaptive_spatial"],
    "temporal_extent_coverage": ["temporal_extent","temporal_coverage","time_period","time_extent"],
    "time_steps_temporal_resolution": ["time_steps","temporal_resolution","time_step","timestep"],
    "variable_temporal_resolution": ["variable_temporal_resolution","adaptive_temporal"],
    # Computational
    "error_handling": ["error_handling","errors","exceptions"],
    "integration_pattern": ["integration.pattern","integration_pattern","pattern"],  # AB explicit
    "communication_mechanism": ["communication","io_mechanism","interaction_style","binding","interface"],
    "execution_instructions": ["execution_instructions","run_instructions","how_to_run","runbook"],
    "acknowledgment_protocols": ["acknowledgment_protocols","ack_protocols","acknowledgement_protocols"],
    "execution_constraints": ["execution_constraints","ordering","timing_constraints"],
    # Engineering
    "parallel_execution": ["parallel_execution","concurrency","parallelism"],
    "latency_expectations": ["latency_expectations","latency","sla"],
    "data_synchronization": ["data_synchronization","sync_strategy","synchronization"],
    # Technology
    "programming_language": ["programming_language","language","lang"],
    "availability_of_source_code": ["availability_of_source_code","source_code","repo","repository"],
    "implementation_verification": ["implementation_verification","tests","verification"],
    "software_specification_and_requirements": ["software_specification_and_requirements","software_requirements","software_stack","dependencies"],
    "hardware_specification_and_requirements": ["hardware_specification_and_requirements","hardware_requirements","hardware","resources"],
    "license": ["license","licence"],
    "landing_page": ["landing_page","homepage","url"],
    "distribution_version": ["distribution_version","package_version","release_version"],
    "file_formats": ["file_formats","formats","file_format"],
    # IO (kept for legacy logic)
    "input": ["input"],
    "output": ["output"],
    "integrated_input": ["integrated_input"],
}

def dig_for_keypaths(d: Any, path: str) -> List[Any]:
    """
    Retrieve values by a dotted path (supports one level of dict nesting).
    Example: "integration.pattern" -> d["integration"]["pattern"]
    """
    parts = path.split(".")
    cur = d
    for p in parts:
        if isinstance(cur, dict) and p in cur:
            cur = cur[p]
        else:
            return []
    return [cur]

def collect_values(root: Dict[str, Any], alias_list: List[str]) -> List[Any]:
    vals: List[Any] = []
    for ali in alias_list:
        if "." in ali:
            vals.extend(dig_for_keypaths(root, ali))
        elif ali in root:
            vals.append(root[ali])
        # also search recursively for keys with same normalized name
        nk_target = normkey(ali)
        def rec(x: Any):
            if isinstance(x, dict):
                for k, v in x.items():
                    if normkey(k) == nk_target:
                        vals.append(v)
                    rec(v)
            elif isinstance(x, list):
                for it in x:
                    rec(it)
        rec(root)
    return vals

def extract_field(model: ModelMeta, key: str) -> List[str]:
    alis = FIELD_ALIASES.get(key, [key])
    vals = collect_values(model.root, alis)
    flat = []
    for v in vals:
        if key in ("input","output"):
            # keep IO handled elsewhere
            continue
        if isinstance(v, dict):
            for vv in v.values():
                if isinstance(vv, (str,int,float)):
                    flat.append(str(vv))
        elif isinstance(v, list):
            for it in v:
                if isinstance(it, (str,int,float)):
                    flat.append(str(it))
                elif isinstance(it, dict):
                    for vv in it.values():
                        if isinstance(vv, (str,int,float)):
                            flat.append(str(vv))
        elif isinstance(v, (str,int,float)):
            flat.append(str(v))
    return dedupe([normalize_phrase(x) for x in flat if str(x).strip()])

# ============================================================
# IO extraction (as in your original, with a few guards)
# ============================================================

def coerce_var_list(v: Any) -> List[str]:
    if isinstance(v, str):
        return split_variables(v)
    if isinstance(v, list):
        out: List[str] = []
        for item in v:
            if isinstance(item, str):
                out.extend(split_variables(item))
            elif isinstance(item, dict):
                # try common keys
                for k in ("name","variable","var","id","description","label"):
                    if k in item and isinstance(item[k], str):
                        out.extend(split_variables(item[k]))
        return dedupe(out)
    if isinstance(v, dict):
        out: List[str] = []
        for k in ("variables","names","list","description"):
            if k in v and isinstance(v[k], (str,list)):
                out.extend(coerce_var_list(v[k]))
        return dedupe(out)
    return []

def coerce_units_list(v: Any) -> List[str]:
    if isinstance(v, str):
        return [u.strip() for u in re.split(r"[;,]", v) if u.strip()]
    if isinstance(v, list):
        return [str(u).strip() for u in v if str(u).strip()]
    if isinstance(v, dict):
        out = []
        for k in ("units","unit"):
            if k in v:
                out.extend(coerce_units_list(v[k]))
        return out
    return []

def parse_ab_pattern(r: Dict[str, Any]) -> str:
    candidates: List[str] = []
    integ = r.get("integration")
    if isinstance(integ, dict):
        pat = integ.get("pattern")
        if isinstance(pat, str):
            candidates.append(pat)
    pp = r.get("purpose_pattern")
    if isinstance(pp, dict):
        pat = pp.get("pattern")
        if isinstance(pat, str):
            candidates.append(pat)

    text = " ".join(candidates).lower()
    if "embedded" in text:
        return "Embedded"
    if "integrated" in text or "tight" in text:
        return "Integrated"
    if "shared" in text:
        return "Shared"
    if "loose" in text:
        return "Loose"
    if "one-way" in text or "one way" in text:
        return "One-Way"
    if "tool" in text or "coupling" in text:
        return "Tool-Coupling"
    return ""

def extract_io(model: ModelMeta, debug: bool = False) -> None:
    r = model.root if isinstance(model.root, dict) else {}
    # Outputs
    out_vars, out_units = [], []
    if isinstance(r.get("output"), dict):
        oo = r["output"]
        out_vars = coerce_var_list(oo.get("variables"))
        out_units = coerce_units_list(oo.get("units"))
    if not out_vars and isinstance(r.get("data"), dict):
        d = r["data"]
        out_vars = coerce_var_list(d.get("variables"))
        out_units = coerce_units_list(d.get("units"))

    # Inputs
    in_vars, in_units = [], []
    if isinstance(r.get("input"), dict):
        ii = r["input"]
        in_vars.extend(coerce_var_list(ii.get("variables")))
        in_units.extend(coerce_units_list(ii.get("units")))
        if isinstance(ii.get("description"), str):
            in_vars.extend(split_variables(ii["description"]))

    if isinstance(r.get("integrated_input"), list):
        for item in r["integrated_input"]:
            if isinstance(item, dict):
                in_vars.extend(split_variables(item.get("description", "")))

    if isinstance(r.get("datasets"), list):
        for ds in r["datasets"]:
            if isinstance(ds, dict) and isinstance(ds.get("dependencies"), list):
                for dep in ds["dependencies"]:
                    if isinstance(dep, str):
                        in_vars.extend(split_variables(dep))

    model.outputs = IOSchema(variables=dedupe(out_vars), units=dedupe(out_units))
    model.inputs  = IOSchema(variables=dedupe(in_vars), units=dedupe(in_units))

    # AB extras
    if model.role == "AB":
        model.ab_pattern = parse_ab_pattern(r)
        if isinstance(r.get("input"), dict):
            model.ab_in = dedupe(coerce_var_list(r["input"].get("variables")))
        if isinstance(r.get("output"), dict):
            model.ab_out = dedupe(coerce_var_list(r["output"].get("variables")))
        canon: List[str] = []
        if isinstance(r.get("integrated_input"), list):
            for item in r["integrated_input"]:
                if isinstance(item, dict):
                    canon.extend(split_variables(item.get("description", "")))
        canon.extend(model.ab_in)
        canon.extend(model.ab_out)
        if not canon:
            for k in ("description", "purpose_pattern", "integration"):
                val = r.get(k, "")
                if isinstance(val, dict):
                    for vv in val.values():
                        if isinstance(vv, str):
                            canon.extend(split_variables(vv))
                elif isinstance(val, str):
                    canon.extend(split_variables(val))
        model.canonical_exchange = dedupe(canon)

    # Populate generic fields bag
    for k in FIELD_ALIASES.keys():
        if k in ("input","output","integrated_input"):
            continue
        model.fields[k] = extract_field(model, k)

    if debug:
        print(f"\n[DEBUG] {model.file} ({model.role})")
        print(f"  outputs: {model.outputs.variables or '(none)'}")
        print(f"  inputs:  {model.inputs.variables or '(none)'}")
        if model.role == "AB":
            print(f"  AB in:   {model.ab_in or '(none)'}")
            print(f"  AB out:  {model.ab_out or '(none)'}")
            print(f"  AB pat:  {model.ab_pattern or '(unspecified)'}")
            print(f"  AB canon:{model.canonical_exchange or '(none)'}")

# ============================================================
# Pattern inference (kept from your original, simplified a bit)
# ============================================================

INMEMORY_KEYS = ["in-memory", "in memory", "shared memory"]
TOOL_KEYS = ["fabm", "esmf", "oasis", "oasis-mct", "mct", "coupler framework", "framework", "plugin", "mediator"]
SYNC_KEYS = ["every physics step", "every step", "synchronous", "same timestep", "same time step"]
FILE_KEYS = ["netcdf", "csv", "file", "files", "post-run", "offline", "batch"]
REPO_KEYS = ["shared repository", "common data store", "database", "object store"]

def text_from_fields(*vals: Any) -> str:
    parts: List[str] = []
    for v in vals:
        if isinstance(v, str):
            parts.append(v)
        elif isinstance(v, list):
            for it in v:
                if isinstance(it, dict):
                    parts.extend([str(x) for x in it.values() if isinstance(x, str)])
                elif isinstance(it, str):
                    parts.append(it)
        elif isinstance(v, dict):
            for x in v.values():
                if isinstance(x, str):
                    parts.append(x)
    return normalize_phrase(" ".join(parts))

def contains_any(text: str, keys: List[str]) -> bool:
    t = text.lower()
    return any(k in t for k in keys)

def detect_integration_pattern(a: ModelMeta, b: ModelMeta, ab: Optional[ModelMeta]) -> Tuple[str, str]:
    if ab and ab.ab_pattern:
        return ab.ab_pattern, f"AB declares pattern '{ab.ab_pattern}'."
    a_text = text_from_fields(a.root)
    b_text = text_from_fields(b.root)
    ab_text = text_from_fields(ab.root if ab else {})
    union_text = " ".join([a_text, b_text, ab_text])

    has_inmem = contains_any(union_text, INMEMORY_KEYS)
    has_tool  = contains_any(union_text, TOOL_KEYS)
    has_sync  = contains_any(union_text, SYNC_KEYS)
    has_file  = contains_any(union_text, FILE_KEYS)
    has_repo  = contains_any(union_text, REPO_KEYS) or ("coupler" in union_text)

    if has_inmem:
        return "Embedded", "Detected in-memory / shared memory phrasing."
    if has_tool:
        return "Tool-Coupling", "Detected coupler/framework keywords."
    if has_sync and has_repo:
        return "Shared", "Detected synchronous cadence with shared store/coupler."
    if has_file and not has_inmem:
        return "Loose", "Detected file-based/API exchange without in-memory."
    return "One-Way", "Defaulted to One-Way (no bidirectional/in-memory/coupler cues)."

# ----- Semantic coverage helpers (Enterprise fields) -----
SEM_COV_MIN = 0.5  # threshold to call the field a Match based on union coverage

def tokens_from_value(s: str) -> Set[str]:
    return tokenize(s) if s else set()

def join_field_values(model: ModelMeta, key: str) -> str:
    return "; ".join(model.fields.get(key, []))

def coverage_vs_ab(src_text: str, ab_text: str) -> Tuple[float, List[str], List[str]]:
    ab_tok = tokens_from_value(ab_text)
    if not ab_tok:
        return 0.0, [], []  # no AB reference; caller will mark Missing
    src_tok = tokens_from_value(src_text)
    covered = sorted(list(ab_tok & src_tok))
    uncovered = sorted(list(ab_tok - src_tok))
    cov = (len(covered) / len(ab_tok)) if ab_tok else 0.0
    return cov, covered, uncovered

def union_coverage_vs_ab(a_text: str, b_text: str, ab_text: str) -> Tuple[float, List[str], List[str]]:
    ab_tok = tokens_from_value(ab_text)
    if not ab_tok:
        return 0.0, [], []
    a_tok = tokens_from_value(a_text)
    b_tok = tokens_from_value(b_text)
    union_tok = a_tok | b_tok
    covered = sorted(list(ab_tok & union_tok))
    uncovered = sorted(list(ab_tok - union_tok))
    cov = (len(covered) / len(ab_tok)) if ab_tok else 0.0
    return cov, covered, uncovered


# ============================================================
# Helpers for “semantic match” rule (at least 1 common variable)
# ============================================================

MIN_VAR_SIM = 0.5

def any_semantic_overlap(src_vars: List[str], dst_vars: List[str], min_sim: float = MIN_VAR_SIM) -> Tuple[bool, List[Tuple[str,str,float]]]:
    score, pairs = list_best_pairwise(src_vars, dst_vars)
    good = [(s,d,sc) for (s,d,sc) in pairs if sc >= min_sim]
    return (len(good) > 0, good)

def pretty_pairs(pairs: List[Tuple[str,str,float]]) -> str:
    if not pairs:
        return ""
    return "; ".join([f"{s} ↔ {d} ({sc:.2f})" for (s,d,sc) in pairs])

# ============================================================
# Bottleneck checkers
#   Each returns a list[dict] (rows) for CSV.
#   Row schema: group,bottleneck,field,pattern,required_check,A_value,B_value,AB_value,detail,result,debug
# ============================================================

def row(group: str, bottleneck: str, field: str, pattern: str, required_check: str,
        av: str, bv: str, abv: str, detail: str, result: str, debug: str="") -> Dict[str,Any]:
    return {
        "group": group,
        "bottleneck": bottleneck,
        "field": field,
        "pattern": pattern or "(unspecified)",
        "required_check": required_check,
        "A_value": av or "",
        "B_value": bv or "",
        "AB_value": abv or "",
        "detail": detail or "",
        "result": result
    }

# ---------- Enterprise ----------

def check_semantic_mismatch(group: str, a: ModelMeta, b: ModelMeta, ab: Optional[ModelMeta], pattern: str) -> List[Dict[str,Any]]:
    def check_semantic_mismatch(group: str, a: ModelMeta, b: ModelMeta, ab: Optional[ModelMeta], pattern: str) -> List[
        Dict[str, Any]]:
        # Enterprise fields to assess
        fields = [
            ("Title", "title"),
            ("Model Version", "model_version"),
            ("Description", "description"),
            ("Keywords", "keywords"),
            ("Model Type", "model_type"),
            ("Scope", "scope"),
            ("Purpose & Pattern", "purpose_pattern"),
            ("Assumptions", "assumptions"),
        ]
        rows: List[Dict[str, Any]] = []

        for label, key in fields:
            A_val = join_field_values(a, key)
            B_val = join_field_values(b, key)
            AB_val = join_field_values(ab, key) if ab else ""

            # Handle Model Version specially: exact equality to AB
            if key == "model_version":
                if not AB_val:
                    res = "Missing"
                    detail = "AB version not provided."
                    rows.append(
                        row(group, "Semantic Mismatch", label, pattern, "Exact equality to AB", A_val, B_val, AB_val,
                            detail, res))
                    continue
                a_ok = (A_val == AB_val) if A_val else False
                b_ok = (B_val == AB_val) if B_val else False
                u_ok = a_ok or b_ok
                detail = f"A==AB:{'✓' if a_ok else '×'}; B==AB:{'✓' if b_ok else '×'}"
                res = "Match" if u_ok else ("Missing" if not A_val and not B_val else "Mismatch")
                rows.append(
                    row(group, "Semantic Mismatch", label, pattern, "A or B must equal AB exactly", A_val, B_val,
                        AB_val, detail, res))
                continue

            # Token coverage against AB
            if not AB_val:
                rows.append(row(group, "Semantic Mismatch", label, pattern, "Coverage vs AB", A_val, B_val, AB_val,
                                "AB value missing.", "Missing"))
                continue

            covA, coveredA, _unA = coverage_vs_ab(A_val, AB_val)
            covB, coveredB, _unB = coverage_vs_ab(B_val, AB_val)
            covU, coveredU, unU = union_coverage_vs_ab(A_val, B_val, AB_val)

            detail = (f"A→AB:{covA:.2f} (covered: {', '.join(coveredA) or '—'}) | "
                      f"B→AB:{covB:.2f} (covered: {', '.join(coveredB) or '—'}) | "
                      f"A∪B→AB:{covU:.2f} (covered: {', '.join(coveredU) or '—'}) | "
                      f"uncovered_AB: {', '.join(unU) or '—'}")
            res = "Match" if covU >= SEM_COV_MIN else "Mismatch"

            rows.append(row(
                group, "Semantic Mismatch", label, pattern,
                f"Coverage threshold ≥ {SEM_COV_MIN:.2f}",
                A_val, B_val, AB_val, detail, res
            ))

        return rows


def check_conceptual_quality_gap(group: str, a: ModelMeta, b: ModelMeta, ab: Optional[ModelMeta], pattern: str) -> List[Dict[str,Any]]:
    keys = [
        ("Links to Publications & Reports","links_to_publications_and_reports"),
        ("Authors’ Unique Identifier","authors_unique_identifier"),
        ("Conceptual Model Evaluation","conceptual_model_evaluation"),
        ("Calibration Tools/Data","calibration_tools_data"),
        ("Validation Capabilities","validation_capabilities"),
        ("Sensitivity Analysis","sensitivity_analysis"),
        ("Uncertainty Analysis","uncertainty_analysis"),
    ]
    rows=[]
    for label,key in keys:
        av = "; ".join(a.fields.get(key, []))
        bv = "; ".join(b.fields.get(key, []))
        # At least one present in each model
        if av and bv:
            res, det = "Match","Both provide metadata."
        elif not av and not bv:
            res, det = "Missing","Neither provides metadata."
        else:
            res, det = "Mismatch","Only one provides metadata."
        rows.append(row(group,"Conceptual Quality Gap",label,pattern,"Pattern-agnostic",av,bv,"",det,res))
    return rows

# ---------- Information ----------

def check_information_viewpoint(group: str, a: ModelMeta, b: ModelMeta, ab: Optional[ModelMeta], pattern: str) -> List[Dict[str,Any]]:
    rows: List[Dict[str,Any]] = []
    # Data Schema Mismatch (semantic overlap rules per pattern)
    # Build values:
    Aout, Bin = a.outputs.variables, b.inputs.variables
    Bout, Ain = b.outputs.variables, a.inputs.variables
    ABin, ABout = (ab.ab_in if ab else []), (ab.ab_out if ab else [])
    # One-Way: A.out ↔ B.in ; A.in ↔ AB.in ; B.out ↔ AB.out
    # Loose/Shared: both directions; and (A or B).in ↔ AB.in ; (A or B).out ↔ AB.out
    # Integrated/Embedded: at least one direction; and (A or B).in ↔ AB.in AND (A or B).out ↔ AB.out
    patt = pattern or ""
    def overlap_bool(src, dst):
        ok, pairs = any_semantic_overlap(src, dst)
        return ok, pairs
    def val_str(v: List[str]) -> str:
        return "; ".join(v) if v else "(none)"
    # Directional checks
    a2b_ok, a2b_pairs = overlap_bool(Aout, Bin)
    b2a_ok, b2a_pairs = overlap_bool(Bout, Ain)
    # AB alignment
    Ain_ABin_sim, _ = list_best_pairwise(a.inputs.variables, ABin) if ABin else (0.0, [])
    Bin_ABin_sim, _ = list_best_pairwise(b.inputs.variables, ABin) if ABin else (0.0, [])
    Aout_ABout_sim, _ = list_best_pairwise(a.outputs.variables, ABout) if ABout else (0.0, [])
    Bout_ABout_sim, _ = list_best_pairwise(b.outputs.variables, ABout) if ABout else (0.0, [])
    # Decide requirements per pattern
    reqs: List[Tuple[str,bool,str]] = []  # (label, pass?, details)
    if patt == "One-Way":
        reqs.append(("A.output ↔ B.input (≥1 semantic match)", a2b_ok, pretty_pairs(a2b_pairs)))
        reqs.append(("A.input ↔ AB.input (similarity>0)", Ain_ABin_sim>0, f"sim={Ain_ABin_sim:.2f}"))
        reqs.append(("B.output ↔ AB.output (similarity>0)", Bout_ABout_sim>0, f"sim={Bout_ABout_sim:.2f}"))
    elif patt in ("Loose","Shared"):
        reqs.append(("A.output ↔ B.input (≥1)", a2b_ok, pretty_pairs(a2b_pairs)))
        reqs.append(("B.output ↔ A.input (≥1)", b2a_ok, pretty_pairs(b2a_pairs)))
        reqs.append(("(A or B).input ↔ AB.input (similarity>0)", max(Ain_ABin_sim,Bin_ABin_sim)>0,
                    f"max={max(Ain_ABin_sim,Bin_ABin_sim):.2f}"))
        reqs.append(("(A or B).output ↔ AB.output (similarity>0)", max(Aout_ABout_sim,Bout_ABout_sim)>0,
                    f"max={max(Aout_ABout_sim,Bout_ABout_sim):.2f}"))
    else:  # Integrated / Embedded / Tool-Coupling / unspecified -> adopt Integrated/Embedded rule
        dir_ok = a2b_ok or b2a_ok
        reqs.append(("A.output ↔ B.input OR B.output ↔ A.input (≥1 in any direction)", dir_ok,
                     f"A2B:{'✓' if a2b_ok else '×'}; B2A:{'✓' if b2a_ok else '×'}"))
        reqs.append(("(A or B).input ↔ AB.input (similarity>0)", max(Ain_ABin_sim,Bin_ABin_sim)>0,
                    f"max={max(Ain_ABin_sim,Bin_ABin_sim):.2f}"))
        reqs.append(("(A or B).output ↔ AB.output (similarity>0)", max(Aout_ABout_sim,Bout_ABout_sim)>0,
                    f"max={max(Aout_ABout_sim,Bout_ABout_sim):.2f}"))

    # Emit rows for Data Schema Mismatch
    rows.append(row(group,"Data Schema Mismatch","A.output",pattern,reqs[0][0], val_str(Aout), val_str(Bin), "", reqs[0][2], "Match" if reqs[0][1] else "Mismatch"))
    if patt == "One-Way":
        rows.append(row(group,"Data Schema Mismatch","A.input vs AB.input",pattern,reqs[1][0],
                        val_str(a.inputs.variables), "", val_str(ABin) or "(not specified)", reqs[1][2], "Match" if reqs[1][1] else "Mismatch"))
        rows.append(row(group,"Data Schema Mismatch","B.output vs AB.output",pattern,reqs[2][0],
                        "", val_str(b.outputs.variables), val_str(ABout) or "(not specified)", reqs[2][2], "Match" if reqs[2][1] else "Mismatch"))
    elif patt in ("Loose","Shared"):
        rows.append(row(group,"Data Schema Mismatch","B.output",pattern,reqs[1][0], val_str(Bout), val_str(Ain), "", reqs[1][2], "Match" if reqs[1][1] else "Mismatch"))
        rows.append(row(group,"Data Schema Mismatch","(A or B).input vs AB.input",pattern,reqs[2][0],
                        f"A.in:{val_str(a.inputs.variables)} | B.in:{val_str(b.inputs.variables)}", "", val_str(ABin) or "(not specified)", reqs[2][2],
                        "Match" if reqs[2][1] else "Mismatch"))
        rows.append(row(group,"Data Schema Mismatch","(A or B).output vs AB.output",pattern,reqs[3][0],
                        f"A.out:{val_str(Aout)} | B.out:{val_str(Bout)}", "", val_str(ABout) or "(not specified)", reqs[3][2],
                        "Match" if reqs[3][1] else "Mismatch"))
    else:
        rows.append(row(group,"Data Schema Mismatch","Direction (any)",pattern,reqs[0][0],
                        f"A.out:{val_str(Aout)}", f"B.out:{val_str(Bout)}", "", reqs[0][2], "Match" if reqs[0][1] else "Mismatch"))
        rows.append(row(group,"Data Schema Mismatch","(A or B).input vs AB.input",pattern,reqs[1][0],
                        f"A.in:{val_str(a.inputs.variables)} | B.in:{val_str(b.inputs.variables)}", "", val_str(ABin) or "(not specified)", reqs[1][2],
                        "Match" if reqs[1][1] else "Mismatch"))
        rows.append(row(group,"Data Schema Mismatch","(A or B).output vs AB.output",pattern,reqs[2][0],
                        f"A.out:{val_str(Aout)} | B.out:{val_str(Bout)}", "", val_str(ABout) or "(not specified)", reqs[2][2],
                        "Match" if reqs[2][1] else "Mismatch"))

    # Other Information bottlenecks (semantic alignment of common exchanged variables only)
    info_simple = [
        ("Temporal Resolution Mismatch","time_steps_temporal_resolution"),
        ("Temporal Coverage Mismatch","temporal_extent_coverage"),
        ("Spatial Resolution Mismatch","spatial_resolution"),
        ("Spatial Coverage Mismatch","spatial_extent_coverage"),
        ("Dimensionality Mismatch","dimensionality"),
    ]
    for label, key in info_simple:
        av = "; ".join(a.fields.get(key, []))
        bv = "; ".join(b.fields.get(key, []))
        abv = "; ".join((ab.fields.get(key, []) if ab else []))
        # Compare only for variables that are exchanged -> we approximate by requiring A and B to have token-sim > 0 if both provided.
        if av and bv:
            sim = jaccard_token_similarity(av, bv)
            res = "Match" if sim > 0 else "Mismatch"
            det = f"token-sim={sim:.2f} (>0 ⇒ semantically aligned)."
        elif not av and not bv:
            res, det = "Missing","Neither provides metadata."
        else:
            res, det = "Mismatch","Only one provides metadata."
        rows.append(row(group, label, key, pattern, "Common exchanged variables must align semantically", av, bv, abv, det, res))
    return rows

# ---------- Computational ----------

def check_computational(group: str, a: ModelMeta, b: ModelMeta, ab: Optional[ModelMeta], pattern: str) -> List[Dict[str,Any]]:
    rows: List[Dict[str,Any]] = []
    # Communication Mechanism Mismatch (file/api/shared memory etc.)
    key = "communication_mechanism"
    av = "; ".join(a.fields.get(key, []))
    bv = "; ".join(b.fields.get(key, []))
    abv = "; ".join((ab.fields.get(key, []) if ab else []))
    # Pattern-based expectation:
    if pattern == "One-Way":
        # accept producer->consumer readable interface; we approximate by requiring non-empty for at least producer side (A) or AB guidance
        ok = bool(av or abv or bv)
        det = "At least one declared IO mechanism present."
        rows.append(row(group,"Communication Mechanism Mismatch","communication_mechanism",pattern,"Producer exposes readable interface",av,bv,abv,det,"Match" if ok else "Mismatch"))
    elif pattern in ("Loose","Shared","Tool-Coupling"):
        # both directions or canonical interface expected -> require A and B non-empty OR AB provides canonical
        ok = (bool(av) and bool(bv)) or bool(abv)
        det = "Both sides or AB canonical interface defined."
        rows.append(row(group,"Communication Mechanism Mismatch","communication_mechanism",pattern,"Both directions bridged or AB canonical",av,bv,abv,det,"Match" if ok else "Mismatch"))
    else:  # Integrated/Embedded
        # prefer in-memory or tight API; approximate by tokens “shared memory”, “in-memory”, “api”, or AB declares same
        joined = " ".join([av,bv,abv]).lower()
        ok = any(k in joined for k in ["in-memory","in memory","shared memory","api","ffi"])
        det = "Expect tight/in-memory/API binding."
        rows.append(row(group,"Communication Mechanism Mismatch","communication_mechanism",pattern,"Tight/in-memory binding present",av,bv,abv,det,"Match" if ok else "Mismatch"))

    # Error Handling
    key = "error_handling"
    av = "; ".join(a.fields.get(key, []))
    bv = "; ".join(b.fields.get(key, []))
    abv = "; ".join((ab.fields.get(key, []) if ab else []))
    # If AB defines canonical, require A,B present; else require both present
    ok = (bool(av) and bool(bv)) or bool(abv)
    det = "A and B error strategies present or AB canonical provided."
    rows.append(row(group,"Error Handling Mismatch","error_handling",pattern,"Align with AB or both declare",av,bv,abv,det,"Match" if ok else "Mismatch"))

    # Execution Instructions (pattern-agnostic: presence)
    key = "execution_instructions"
    av = "; ".join(a.fields.get(key, []))
    bv = "; ".join(b.fields.get(key, []))
    okA, okB = bool(av), bool(bv)
    rows.append(row(group,"Execution Instruction Gap","execution_instructions",pattern,"Presence per component",av,"","", "A provides run instructions", "Match" if okA else "Missing"))
    rows.append(row(group,"Execution Instruction Gap","execution_instructions",pattern,"Presence per component","",bv,"", "B provides run instructions", "Match" if okB else "Missing"))

    return rows

# ---------- Engineering ----------

def check_engineering(group: str, a: ModelMeta, b: ModelMeta, ab: Optional[ModelMeta], pattern: str) -> List[Dict[str,Any]]:
    rows: List[Dict[str,Any]] = []

    def presence_align(label: str, key: str, req: str):
        av = "; ".join(a.fields.get(key, []))
        bv = "; ".join(b.fields.get(key, []))
        abv = "; ".join((ab.fields.get(key, []) if ab else []))
        # If AB canonical exists, accept if A or B present; else require both present.
        if abv:
            ok = bool(av or bv)
            det = "AB canonical present; at least one component declares compatible metadata."
        else:
            ok = bool(av and bv)
            det = "Require both components to declare compatible metadata."
        rows.append(row(group,label,key,pattern,req,av,bv,abv,det,"Match" if ok else "Mismatch"))

    presence_align("Parallel Execution Incompatibility","parallel_execution",
                   "Declare concurrency/parallel parameters aligned with AB")
    presence_align("Execution Constraint Mismatch","execution_constraints",
                   "Declare ordering/timing constraints aligned with AB")
    presence_align("Acknowledgment Protocol Mismatch","acknowledgment_protocols",
                   "Declare ack rules aligned with AB")
    presence_align("Latency Expectation Mismatch","latency_expectations",
                   "Declare latency expectations aligned with AB")

    # Data Synchronization (if present in metadata)
    key = "data_synchronization"
    av = "; ".join(a.fields.get(key, []))
    bv = "; ".join(b.fields.get(key, []))
    abv = "; ".join((ab.fields.get(key, []) if ab else []))
    if av or bv or abv:
        if abv:
            ok = bool(av or bv)
            det = "AB synchronization present; at least one component aligns."
        else:
            ok = bool(av and bv)
            det = "Require both components to declare synchronization."
        rows.append(row(group,"Data Synchronization","data_synchronization",pattern,"Synchronization strategy present",av,bv,abv,det,"Match" if ok else "Mismatch"))

    return rows

# ---------- Technology ----------

def check_technology(group: str, a: ModelMeta, b: ModelMeta, ab: Optional[ModelMeta], pattern: str) -> List[Dict[str,Any]]:
    rows: List[Dict[str,Any]] = []

    def eq_or_token(label: str, key: str, req: str, exact: bool=False):
        av = "; ".join(a.fields.get(key, []))
        bv = "; ".join(b.fields.get(key, []))
        abv = "; ".join((ab.fields.get(key, []) if ab else []))
        if not av and not bv:
            res, det = "Missing","Neither provides metadata."
        elif exact:
            if av and bv and av == bv:
                res, det = "Match","Exact equality."
            else:
                res, det = "Mismatch","Exact equality required."
        else:
            sim = jaccard_token_similarity(av, bv) if av and bv else 0.0
            if abv:
                # if AB canonical exists, accept if either A or B is similar to AB
                simA = jaccard_token_similarity(av, abv) if av else 0.0
                simB = jaccard_token_similarity(bv, abv) if bv else 0.0
                ok = (simA>0 or simB>0)
                res, det = ("Match" if ok else "Mismatch", f"A↔AB={simA:.2f}, B↔AB={simB:.2f} (>0 ⇒ aligned)")
            else:
                res, det = ("Match" if sim>0 else "Mismatch", f"A↔B token-sim={sim:.2f} (>0 ⇒ aligned)")
        rows.append(row(group,label,key,pattern,req,av,bv,abv,det,res))

    eq_or_token("Programming Language Incompatibility","programming_language","Languages compatible (binding available)")
    # Availability of Source Code: presence check
    for who, model in (("A",a),("B",b)):
        key="availability_of_source_code"
        vv = "; ".join(model.fields.get(key, []))
        rows.append(row(group,"Source Code Availability Gap",f"{key} ({who})",pattern,"Presence of accessible source", vv,"","",f"{who} provides code location","Match" if vv else "Missing"))
    # Implementation Verification: presence
    for who, model in (("A",a),("B",b)):
        key="implementation_verification"
        vv = "; ".join(model.fields.get(key, []))
        rows.append(row(group,"Implementation Verification Gap",f"{key} ({who})",pattern,"Presence of tests/verification", vv,"","",f"{who} provides verification","Match" if vv else "Missing"))

    eq_or_token("Software Environment Mismatch","software_specification_and_requirements","Stacks compatible or aligned with AB")
    eq_or_token("Hardware Resource Mismatch","hardware_specification_and_requirements","Hardware profiles aligned with AB")
    eq_or_token("Distribution Version Mismatch","distribution_version","Same/approved versions", exact=False)
    eq_or_token("File Format Mismatch","file_formats","Supported/approved formats", exact=False)

    # License Incompatibility: presence + rough equality if both present
    key="license"
    av = "; ".join(a.fields.get(key, []))
    bv = "; ".join(b.fields.get(key, []))
    if av and bv:
        sim = jaccard_token_similarity(av, bv)
        det = f"A↔B token-sim={sim:.2f}"
        res = "Match" if sim>0 else "Mismatch"
    elif not av and not bv:
        det, res = "Neither provides license.","Missing"
    else:
        det, res = "Only one provides license.","Mismatch"
    rows.append(row(group,"License Incompatibility","license",pattern,"Compatible licensing",av,bv,"",det,res))

    # Landing page: presence
    for who, model in (("A",a),("B",b)):
        key="landing_page"
        vv = "; ".join(model.fields.get(key, []))
        rows.append(row(group,"Landing Page Gap",f"{key} ({who})",pattern,"Reachable landing/home page", vv,"","",f"{who} provides landing page","Match" if vv else "Missing"))

    return rows

# ============================================================
# Group evaluation
# ============================================================

def evaluate_group(gid: str, A: ModelMeta, B: ModelMeta, AB: Optional[ModelMeta], debug: bool=False) -> List[Dict[str,Any]]:
    pattern, pr = detect_integration_pattern(A, B, AB)
    if debug:
        print(f"[GROUP {gid}] inferred pattern: {pattern} | {pr}")
    rows: List[Dict[str,Any]] = []
    rows += check_semantic_mismatch(gid, A, B, AB, pattern)
    rows += check_conceptual_quality_gap(gid, A, B, AB, pattern)
    rows += check_information_viewpoint(gid, A, B, AB, pattern)
    rows += check_computational(gid, A, B, AB, pattern)
    rows += check_engineering(gid, A, B, AB, pattern)
    rows += check_technology(gid, A, B, AB, pattern)
    return rows

# ============================================================
# CLI main
# ============================================================

def main(argv: List[str]) -> None:
    debug = False
    args: List[str] = []
    for a in argv:
        if a == "--debug":
            debug = True
        else:
            args.append(a)

    paths: List[str] = []
    default_root = "metamodel" if os.path.isdir("metamodel") else "."
    scan_roots = args or [default_root]
    for a in scan_roots:
        if os.path.isdir(a):
            for root, _, files in os.walk(a):
                for fn in files:
                    if fn.lower().endswith((".yml",".yaml")):
                        paths.append(os.path.join(root, fn))
        else:
            if a.lower().endswith((".yml",".yaml")):
                paths.append(a)

    if not paths:
        print("No YAML files found.")
        return

    models = load_yaml_models(paths)
    for m in models:
        try:
            extract_io(m, debug=debug)
        except Exception as e:
            print(f"Warning: failed to extract IO for {m.file}: {e}")

    groups: Dict[str, List[ModelMeta]] = defaultdict(list)
    for m in models:
        groups[m.group].append(m)

    # Evaluate groups
    report_rows: List[Dict[str,Any]] = []
    for gid, items in sorted(groups.items()):
        A = next((x for x in items if x.role == "A"), None)
        B = next((x for x in items if x.role == "B"), None)
        ABs = [x for x in items if x.role == "AB"]
        AB = ABs[0] if ABs else None
        if A and B:
            try:
                report_rows.extend(evaluate_group(gid, A, B, AB, debug=debug))
            except Exception as e:
                print(f"Warning: failed to evaluate group {gid}: {e}")

    if not report_rows:
        print("No A/B pairs found to score.")
        return

    df = pd.DataFrame(report_rows)
    out_csv = "match_report.csv"
    df.to_csv(out_csv, index=False)
    # Compact console view
    preview_cols = ["group","bottleneck","field","pattern","result"]
    print(df[preview_cols].to_string(index=False))
    print("\nSaved CSV:", out_csv)

if __name__ == "__main__":
    main(sys.argv[1:])
